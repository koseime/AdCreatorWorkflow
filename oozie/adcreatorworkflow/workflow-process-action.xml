<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns="uri:oozie:workflow:0.2" name="parse-catalog-wf">
    <start to="parse-catalog"/>

    <action name='parse-catalog'>

        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${homeRoot}/${outputDir}"/>
            </prepare>

            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>com.kosei.adcreatorworkflow.hadoop.GoogleTextParserMapper</value>
                </property>

                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.TextInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.hadoop.io.NullWritable</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>com.kosei.adcreatorworkflow.hadoop.io.AdCreatorAssetsWritable</value>
                </property>

                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>${inputDir}</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>${outputDir}</value>
                </property>

            </configuration>
        </map-reduce>
        <ok to="crawl-catalog"/>
        <error to="fail"/>
    </action>

    <action name='crawl-catalog'>


        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${homeRoot}/${outputCrawlDir}"/>
            </prepare>



            <configuration>
                <property>
                    <name>mapred.mapper.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.reducer.new-api</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
                <property>
                    <name>mapreduce.job.map.class</name>
                    <value>com.kosei.adcreatorworkflow.hadoop.ByteWritableImageCrawlerMapper</value>
                </property>

                <property>
                    <name>mapreduce.job.inputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.outputformat.class</name>
                    <value>org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat</value>
                </property>
                <property>
                    <name>mapreduce.job.output.key.class</name>
                    <value>org.apache.hadoop.io.NullWritable</value>
                </property>
                <property>
                    <name>mapreduce.job.output.value.class</name>
                    <value>org.apache.hadoop.io.BytesWritable</value>
                </property>

                <property>
                    <name>mapreduce.input.fileinputformat.inputdir</name>
                    <value>${outputDir}</value>
                </property>
                <property>
                    <name>mapreduce.output.fileoutputformat.outputdir</name>
                    <value>${outputCrawlDir}</value>
                </property>

            </configuration>
        </map-reduce>
        <ok to="ad-create"/>
        <error to="fail"/>
    </action>


    <action name='ad-create'>




        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${homeRoot}/${outputImageTar}"/>
            </prepare>
            <!--

            /*
 Command to run pipes
 hadoop pipes \
  -Dhadoop.pipes.java.recordreader=true \
  -Dhadoop.pipes.java.recordwriter=true \
  -Dmapred.reduce.tasks=0 \
  -files images/320x50Border.png \
  -input test/input-byteswritable.seq \
  -inputformat org.apache.hadoop.mapred.SequenceFileInputFormat \
  -output output \
  -writer org.apache.hadoop.mapred.SequenceFileOutputFormat \
  -program bin/ad-creator


-->

            <pipes>
                    <inputformat>org.apache.hadoop.mapred.SequenceFileInputFormat</inputformat>
                    <writer>org.apache.hadoop.mapred.SequenceFileOutputFormat</writer>
                    <program>bin/ad-creator#ad-creator</program>
            </pipes>

            <configuration>
                <property>
                    <name>mapred.input.dir</name>
                    <value>${outputCrawlDir}</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${outputImageTar}</value>
                </property>
                <property>
                    <name>mapred.reduce.tasks</name>
                    <value>0</value>
                </property>
                <property>
                    <name>hadoop.pipes.java.recordreader</name>
                    <value>true</value>
                </property>
                <property>
                    <name>hadoop.pipes.java.recordwriter</name>
                    <value>true</value>
                </property>
            </configuration>

            <file>images/320x50Border.png</file>


        </map-reduce>
        <ok to="end"/>
        <error to="fail"/>
    </action>



    <kill name="fail">
        <message>Failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>

    <end name="end"/>


</workflow-app>
